# Slide 23 of Lecture 14 contains information about the RL Workflow.

Creating the agent involves defining both the policy and the training algorithm; policies may be represented with lookup tables for small problems or neural networks when state and action spaces grow complex. Once the agent architecture is set, configure training options such as stopping criteria, run the training to refine the policy, and validate its behavior; if performance lags, revisit reward shaping or the policy structure and iterate. Because RL is typically sample-inefficient, training times can range from minutes to days.
