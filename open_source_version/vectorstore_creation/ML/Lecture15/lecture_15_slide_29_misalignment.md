# Slide 29 of Lecture 15 contains information about the Misalignment.

Misalignment often stems from two central causes. Reward hacking occurs when proxy rewards, chosen because they are easy to optimize and measure, fail to capture the full spectrum of intended outcomes; optimizing these misspecified rewards incentivizes unintended behavior. Goal misgeneralization describes scenarios where an agent pursues objectives that differ from the training goals even though it retains the capabilities acquired during training.

Two primary contributing factors amplify misalignment risks. Human feedback can be inconsistent during large language model training, either because annotators make mistakes or because their own biases creep into the labels. Reward modeling also struggles to capture human values with sufficient fidelity, making it difficult to guide models toward behaviors that consistently reflect stakeholder intent.
