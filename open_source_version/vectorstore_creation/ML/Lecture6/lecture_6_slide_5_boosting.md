# Slide 5 of Lecture 6 contains information about the Boosting.

In boosting, subsequent predictors learn from the mistakes of earlier ones by giving misclassified or high-error observations a higher probability of being used in later models, often using decision trees, regressors, or classifiers as the base learners; gradient boosting is a common variant that can be fast at prediction time but requires careful choice of stopping criteria to avoid overfitting the training data.
