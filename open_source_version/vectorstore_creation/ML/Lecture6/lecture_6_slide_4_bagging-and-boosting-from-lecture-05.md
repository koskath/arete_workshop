# Slide 4 of Lecture 6 contains information about the Bagging and Boosting (from Lecture-05).

Bagging builds many independent predictors and combines them using model-averaging techniques such as weighted averages or majority votes—Random Forests are a classic example that can help control overfitting—whereas boosting builds predictors sequentially, as in Gradient Boosting, which can overfit if not regularized, and in all these methods ensembles work best when the individual predictors are as independent from each other as possible.
