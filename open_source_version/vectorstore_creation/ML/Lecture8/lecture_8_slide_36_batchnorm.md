# Slide 36 of Lecture 8 contains information about the BatchNorm.

Note that if you add a BatchNorm layer as the very first layer of your neural network, you do not need to standardize your training set; the BN layer will do it for you. The vanishing gradients problem can be reduced to a point that activation functions can be used for further solution. BatchNorm can improve many deep neural networks.
