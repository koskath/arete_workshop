# Slide 17 of Lecture 8 contains information about the Variance.

The image provides a detailed overview of Stochastic Gradient (SG) Optimization, defining it as an algorithm designed to Minimize averages of smooth functions, where the function $f(w)$ is typically the average loss over a dataset. The fundamental formula for this average loss is given as $f(w) = \frac{1}{n} \sum_{i=1}^{n} f_i(w)$, where $n$ is the total number of examples and $f_i(w)$ is the loss associated with the individual example $i$. SG is characterized by its Cheap Iterations, which are computationally efficient even when the number of examples '$n$' is very large, because the gradient update at each step is calculated using only one training example '$i$', chosen randomly. This efficiency leads to Noisy updates, where the parameter update rule is given by $w^{t+1} = \alpha^t - \Delta f_i(w)^t$. Here, the estimated gradient $\Delta f_i(w)^t$ is inherently noisy compared to the true gradient of the full average, and $\alpha$ is the learning rate. To ensure proper convergence, a Learning Schedule is necessary: SG is only guaranteed to solve the problem if the learning rate $\alpha^t$ is gradually reduced, or "goes to 0 at an appropriate rate," over time. The overall goal of this randomized, noisy process is to converge to the minimum "on average" across many single steps.
