# Slide 14 of Lecture 8 contains information about the Stochastic Gradient (SG).

Stochastic Gradient (SG) is an Iterative optimization algorithm to minimize averages. The algorithm starts by setting some initial guess, $w^0$. In each step, it generates a new guess by moving in the negative gradient direction, but crucially, this gradient is calculated for a random training example '$i$' only, rather than the entire dataset, as shown by the first update rule: $w^1 = w^0 - \alpha \nabla f_i(w^0)$. The algorithm proceeds to repeat successively refine the parameters using this same approach, with the general update rule being: $w^{t+1} = w^t - \alpha \nabla f_i(w^t)$ for $t = 0, 1, 2, 3, \dots$ . The primary efficiency gain of SG comes from its Gradient Cost, where the Gradient is independent of 'n' (the total number of training examples). Consequently, the Iterations are 'n' times faster than GD iterations, making it a highly scalable approach for optimizing models on very large datasets.
