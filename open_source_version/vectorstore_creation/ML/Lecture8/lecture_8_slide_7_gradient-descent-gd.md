# Slide 7 of Lecture 8 contains information about the Gradient Descent (GD).

The idea behind Gradient Descent is to tweak parameters iteratively to minimize a cost function. It starts with a "guess" w0, then uses gradient ∇f(w0) to generate a better guess w1. The algorithm continues using gradient ∇f(w1) to generate a better guess w2, and uses gradient ∇f(w2) to generate a better guess w3. The limit of wt as 't' goes to ∞ has ∇ f(wt) = 0. It converges to the global optimum if 'f' is convex. Note that gradient in vector analysis is a vector operator denoted ∇ (called del/nabla).
