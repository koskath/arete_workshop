# Slide 4 of Lecture 11 contains information about the Long Short-Term Memory (or LSTM).

Slide 4 explains that LSTM is a recurrent neural network variant featuring a more elaborate memory cell than the standard RNN and was proposed in 1997 by Sepp Hochreiter and Jürgen Schmidhuber*. The architecture contains gates that regulate the information flow within the recurrent cell—specifically the memory cell, input gate, forget gate, and output gate—making it efficient for capturing long-term dependencies across many time instants and enabling faster training when detecting extended temporal patterns.

Common applications include image captioning, stock market prediction, machine translation, and text classification for sequence processing, underscoring why this material is emphasized on slide 4.

*Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory,” Neural Computation 9, no. 8 (1997): 1735–1780.
