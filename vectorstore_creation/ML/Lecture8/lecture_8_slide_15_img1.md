# Here is what the image 1 in slide 15 of lecture 8 contains:

The two graphs effectively illustrate the behavior of the Stochastic Gradient (SG) training algorithm when optimizing models, contrasting its performance on Convex and Non-Convex cost functions.Convex Cost FunctionThe lower graph depicts a Convex cost function, which represents the simplest and most ideal optimization landscape. This function is characterized by a distinct, bowl-like shape and features only a single minimum, labeled as Minimum ($\hat{\theta}$), which is the global optimum. Starting from a Random initial value, the SG algorithm's iterative Learning step reliably and efficiently follows the negative gradient direction, leading to consistent convergence toward this single global optimum. Because there are no other valleys or flat regions, optimization on a convex surface is straightforward and robust.Non-Convex Cost FunctionThe upper graph illustrates a Non-Convex cost function, which is the more complex and common landscape encountered in modern machine learning, particularly with deep neural networks. This function contains multiple valleys, presenting significant challenges for optimization. It includes a single Global minimum (the lowest cost overall), but also features a higher Local minimum where the algorithm might prematurely settle, finding a sub-optimal solution. Furthermore, the curve shows a flat region, or a Plateau, where the gradient is close to zero, causing the learning steps to become very small and optimization progress to stall. Training algorithms like SG on non-convex surfaces must contend with the risk of getting trapped in a sub-optimal local minimum or suffering from extremely slow convergence in plateau regions.In summary, the key difference is the complexity of the optimization path: SG finds the minimum reliably on Convex functions, but must contend with the multiple challenges of Local minimum trapping and slow progress on Plateaus when navigating Non-Convex cost functions.