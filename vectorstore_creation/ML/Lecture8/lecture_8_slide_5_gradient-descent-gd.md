# Slide 5 of Lecture 8 contains information about the Gradient Descent (GD).

Gradient Descent is an optimization algorithm capable of finding optimal solutions to a wide range of problems. While normal equations only solve linear least squares problems and cost O(nd2 + d3), Gradient Descent solves many other problems. Gradient Descent costs O(ndt) to run for 't' iterations and can be faster when 'd' is very large. Note that gradient presents slope.
