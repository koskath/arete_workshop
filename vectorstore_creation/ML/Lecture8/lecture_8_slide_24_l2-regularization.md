# Slide 24 of Lecture 8 contains information about the L2-Regularization.

The intuition is that large slopes w tend to lead to overfitting. L2-regularization minimizes squared error plus a penalty on the L2-norm of 'w'. Gradient measures how steep a slope or a line is, and gradients are calculated by dividing the vertical height by the horizontal distance. It balances getting low error versus having small slopes 'w'. The principle is to "increase training error if it makes 'w' much smaller." This nearly-always reduces overfitting. The regularization parameter λ > 0 controls the "strength" of regularization. Large λ puts a large penalty on slopes. The L2 parameter norm penalty is commonly known as weight decay or Ridge regression. Lambda, which is a scalar value, is called the regularization rate.
