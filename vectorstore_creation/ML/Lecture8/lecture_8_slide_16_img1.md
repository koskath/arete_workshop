# Here is what the image 1 in slide 16 of lecture 8 contains:

The image details the core concepts of Stochastic Gradient (SG) Optimization, an algorithm designed to Minimize averages of smooth functions, typically the average loss function $f(w) = \frac{1}{n} \sum_{i=1}^{n} f_i(w)$, where $n$ is the number of training examples and $f_i(w)$ is the loss for a single example $i$. SG is characterized by Cheap Iterations because, at each step, the gradient update is calculated based on only one randomly chosen training example $i$, making it computationally efficient even when $n$ is very large. This leads to Noisy updates, where the parameter $w$ is adjusted using the rule $w^{t+1} = \alpha^t - \Delta f_i(w)^t$, introducing variance since the gradient $\Delta f_i(w)^t$ is an estimate of the true full gradient. The $\alpha$ term represents the learning rate. Effective convergence relies on a Learning Schedule where SG solves the problem if the learning rate $\alpha^t$ is reduced, or "goes to 0 at an appropriate rate," rather than being a constant value. This approach of using noisy, random steps is intended to produce a good solution "on average" across multiple single steps.