# Slide 39 of Lecture 9 contains information about the Rectified Linear Units (ReLU).

ReLU mitigates the vanishing and exploding gradient problem. ReLU offers improvement over the tanh and sigmoid activation functions. ReLU works by setting the activation to 0 for values x < 0, and setting a linear slope of 1 when values x > 0.
42
Bisong E. (2019) More on Optimization Techniques. In: Building Machine Learning and Deep Learning Models on Google Cloud Platform. Apress, Berkeley, CA
