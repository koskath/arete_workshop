# Slide 6 of Lecture 6 contains information about the Gradient Boosting.

Gradient Boosting can be thought of as Gradient Descent plus Boosting: it works by sequentially adding predictors to an ensemble, each one correcting its predecessor by fitting to the residual errors, then summing all predictorsâ€™ outputs for the final prediction, with popular variants including Adaptive Boosting (AdaBoost) and Extreme Gradient Boosting (XGBoost).
