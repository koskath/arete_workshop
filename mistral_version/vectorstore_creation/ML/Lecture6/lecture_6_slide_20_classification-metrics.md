# Slide 20 of Lecture 6 contains information about the Classification Metrics.

Classification metrics evaluate how good or bad a model’s predictions are when the outputs are discrete, using measures such as accuracy, recall, precision, F1 score (as introduced in Lecture 2, slides 35–37), and AU-ROC, while tools like the confusion matrix summarize prediction outcomes even though they are not themselves metrics.
