# Slide 10 of Lecture 11 contains information about the Gated Recurrent Unit (GRU).

Slide 10 introduces the Gated Recurrent Unit (GRU) as a notable LSTM variant that, along with LSTM cells, underpins much of the success of modern RNNs. A GRU simplifies the LSTM design while still handling much longer sequences than simple RNNs by combining the forget and input gates to decide what should be committed to or discarded from long-term memory, merging the long- and short-term representations into a single state vector h_t, and removing the output gate so that h_t is returned at every time step. The architecture is implemented in `tf.keras.layers.GRU()`, as detailed by Kyunghyun Cho et al. in “Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation,” presented at EMNLP 2014.
