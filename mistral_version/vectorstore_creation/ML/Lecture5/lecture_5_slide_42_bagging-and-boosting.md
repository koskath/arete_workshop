# Slide 42 of Lecture 5 contains information about the Bagging and Boosting.

Bagging builds many independent predictors and combines them using model-averaging techniques such as weighted averages or majority votes—Random Forests are a prime example that can help reduce overfitting—while boosting constructs predictors sequentially, as in Gradient Boosting, which can overfit if not regularized; in general, ensemble methods work best when the individual predictors are as independent from each other as possible.
