# Slide 36 of Lecture 10 contains information about the TensorFlowâ€™s Computation:.

The image is titled TensorFlow's Computation: Dataflow Graph Distribution and illustrates how a TensorFlow computation is distributed across different hardware devices, specifically CPUs & GPUs. The core concept is that the dataflow graph, which defines the sequence of operations, can be split and executed across multiple processors for efficiency. In the diagram, the large blue rectangle represents the CPU environment, while the yellow shaded area represents a dedicated processing unit, labeled GPU 0. The flow shows various operations being assigned to these devices: nodes like Add, an intermediate operation (represented by '...'), and Mul are executed on the GPU 0. Tensors representing biases and a calculated value (represented by '...') are fed into the GPU operations. The learning rate is also shown as an input tensor for the multiplication operation. After processing on the GPU, the result is sent back to the CPU, where a final operation, Assign Sub, takes place, likely for updating parameters like the biases after the gradient calculation. This visualization highlights TensorFlow's capacity for parallel and distributed computing by allocating different parts of the computation graph to specialized hardware.
