# Slide 13 of Lecture 8 contains information about the Stochastic Gradient Descent (SGD).

Deep Neural Networks are trained using stochastic gradient descent. SGD estimates the error gradient for the current state from training data and updates the weights of the model using backpropagation. We use SGD, not really GD. It is an approach to discriminative learning of linear classifiers under convex loss functions such as (linear) SVM and Logistic Regression. Applications include text classification and natural language processing, particularly for sparse machine learning problems. The pros of SGD include efficiency and ease of implementation. The cons are that it needs hyperparameters such as regularization parameter and number of iterations, and it is sensitive to feature scaling.
