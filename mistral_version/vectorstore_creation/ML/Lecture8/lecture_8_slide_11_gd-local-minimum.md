# Slide 11 of Lecture 8 contains information about the GD: Local Minimum.

Gradient Descent (GD) is an optimization technique used to find the **local minimum** of a cost function $f(w)$ by iteratively updating the model parameters '$w$'. The foundational principle of GD relies on the fact that for any given set of parameters, the **direction of the largest decrease** in the function's value is given by the negative of the function's gradient, or $-\nabla f(w)$.  The visualization demonstrates this process on a simplified 1-dimensional function: starting at a point $w^1$ where the slope $\nabla f(w^1)$ is **negative**, the algorithm correctly determines that it must move $w$ to a **more positive** value to decrease $f(w)$. Conversely, if the step overshoots the minimum and lands at a point where the slope $\nabla f(w^t)$ is **positive**, the algorithm recognizes it must move in the **negative direction** to reduce the cost. This iterative movement, always opposing the direction of the local slope, guides the parameters toward the minimum. A critical practical consideration for effective GD convergence is to **ensure all features have a similar scale**, which helps prevent erratic, slow "zigzagging" behavior caused by a highly distorted cost landscape.
