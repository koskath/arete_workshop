# Slide 27 of Lecture 8 contains information about the L1/LASSO Regularization.

L1-regularization simultaneously regularizes and selects features. It automatically performs feature selection and outputs a sparse model, with Î± ranging from 0.001 to 0.1. Both ridge regression and LASSO work for high-dimensional problems, but LASSO is good because it leads to a sparse estimator where many coefficients are 0. Comparing L2-Regularization and L1-Regularization: both are insensitive to data change and both decrease variance. However, L2-Regularization has a closed-form solution, which is a closed form of the general quadratic equation ax2+bx+c = 0, while L1-Regularization requires an iterative solver. L2-Regularization has a unique solution, while L1-Regularization's solution is not unique. In L2-Regularization, all 'w' tend to be non-zero, while in L1-Regularization, many 'w' tend to be zero.
