# Slide 15 of Lecture 12 contains information about the Challenges Hyper-parameter Optimization.

Hyper-parameter optimization is challenging because each function evaluation can be prohibitively expensive for large deep-learning models, intricate pipelines, or massive datasets. The configuration space tends to be high-dimensional and heterogeneous, mixing continuous, categorical, and conditional hyperparameters, so it is rarely obvious which knobs to tune. Complicating matters further, there is no straightforward gradient for the hyperparameter loss, and common optimization assumptions like convexity or smoothness usually fail. Since training datasets are finite, we cannot directly optimize generalization performance, which forces practitioners to rely on indirect validation estimates. 
